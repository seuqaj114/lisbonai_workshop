{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Goal of this workshop\n",
    "The goal of this workshop is to give you a thourough understanding of Tensorflow and how you can use it to effectively build and train neural network models.\n",
    "\n",
    "We will not be running big models since we have limited time and hardware, but hopefully this will give you the skills and tools to go home later and confidently start running different experiments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets start by importing some standard libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "np.set_printoptions(precision=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__take a moment to review numpy and talk about how operations are element-wise, etc__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.0\n"
     ]
    }
   ],
   "source": [
    "x = tf.placeholder(tf.float32, shape=[], name=\"x\")\n",
    "y = x + 2\n",
    "\n",
    "session = tf.Session()\n",
    "\n",
    "result = session.run(y, feed_dict={x: 3})\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since both y and x are nodes in the graph, we can \"ask\" for both of them in the output of session.run (this does not run the graph twice!):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y = 7.0\n",
      "x = 5.0\n"
     ]
    }
   ],
   "source": [
    "result = session.run([y, x], feed_dict={x: 5})\n",
    "print(\"y = {}\".format(result[0]))\n",
    "print(\"x = {}\".format(result[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's say we want a linear model, so we have some input $x$ and the output is given by $y = x^T w + b$. Let's put that into a function called _linear_, which receives an input $x$ and weights $w$ and bias $b$ and returns $y$. Make the input $x$ a 3-dimensional vector and use a constant variable $w$ and $b$ by using tf.constant(value).\n",
    "\n",
    "Given info:\n",
    "    remember that $x^T w = \\sum_i x_i \\cdot w_i$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y = 6.0\n"
     ]
    }
   ],
   "source": [
    "def linear(inp, weights, bias):\n",
    "    return tf.reduce_sum(tf.multiply(weights, inp)) + bias\n",
    "\n",
    "x = tf.placeholder(tf.float32, shape=[3], name=\"x\")\n",
    "w = tf.constant(np.array([1.,1.,1.], dtype=np.float32), name=\"w\")\n",
    "b = tf.constant(0.0, name=\"b\")\n",
    "y = linear(x, w, b)\n",
    "\n",
    "session = tf.Session()\n",
    "\n",
    "result = session.run(y, feed_dict={x: np.array([1.,2.,3.])})\n",
    "print(\"y = {}\".format(result))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok so at this point we have a function that can take one input and return its output according to a simple linear model. However, we usually want to pass many inputs at the same time (this is what we call a batch), and get all the outputs. So let's change our placeholder in order for it to accept 4 input vectors (lets make x a 4x3 placeholder). Remember we now want our output to have 4 elements, one for each input array.\n",
    "__depict this batch multiplication, and show that we need to ue axis=1 in the reduce_sum__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y = [  6.  15.  24.  33.]\n"
     ]
    }
   ],
   "source": [
    "def linear(inp, weights, bias):\n",
    "    return tf.reduce_sum(tf.multiply(weights, inp), axis=1) + bias\n",
    "\n",
    "x = tf.placeholder(tf.float32, shape=[4, 3], name=\"x\")\n",
    "w = tf.constant(np.array([1.,1.,1.], dtype=np.float32), name=\"w\")\n",
    "b = tf.constant(0.0, name=\"b\")\n",
    "y = linear(x, w, b)\n",
    "\n",
    "session = tf.Session()\n",
    "\n",
    "result = session.run(y, feed_dict={x: np.array([[1,2,3], [4,5,6], [7,8,9], [10,11,12]])})\n",
    "print(\"y = {}\".format(result))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Furthermore, we also want the outputs to have higher dimension than 1, so let's make our outputs are 2 dimensional. This means that now $w$ is not a vector but a input_dim x out_dim matrix, so 3x2 in this case, and $b$ is a vector of length 2. So go ahead and change the matrix accordingly and check if the results are correct. Your $y$ now should be 4x2 dimensional.\n",
    "\n",
    "Also, using tf.reduce_sum and tf.multiply is ugly and won't work in this case, so let's use what we should have been using in the first place: tf.matmul, which multiplies two matrices, which is what we want!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y = [[  6.  12.]\n",
      " [ 15.  30.]\n",
      " [ 24.  48.]\n",
      " [ 33.  66.]]\n"
     ]
    }
   ],
   "source": [
    "def linear(inp, weights, bias):\n",
    "    return tf.matmul(inp, weights) + bias\n",
    "\n",
    "x = tf.placeholder(tf.float32, shape=[4, 3], name=\"x\")\n",
    "w = tf.constant(np.array([[1,2],[1,2],[1,2]], dtype=np.float32), name=\"w\")\n",
    "b = tf.constant(np.array([0,0], dtype=np.float32), name=\"b\")\n",
    "y = linear(x, w, b)\n",
    "\n",
    "session = tf.Session()\n",
    "\n",
    "result = session.run(y, feed_dict={x: np.array([[1,2,3], [4,5,6], [7,8,9], [10,11,12]])})\n",
    "print(\"y = {}\".format(result))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see we are heading towards a linear regression implementation here. We have a linear model, our inputs, outputs, and parameters. However, we need a few more things before we can make it learn.\n",
    "\n",
    "First of all we need targets, right? For linear regression we have our inputs, but we also have a set of target outputs, to which the model's outputs $y$ must be close. So let's create another placeholder which will receive the target outputs and compute the squared difference between that and the model's output for each item in the batch. __I will provide some data here__ Remember target outputs are supposed to be the same shape as the model's outputs, so that we can compute their difference. Also, change session.run so that it returns the cost and not the outputs y.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cost = [   45.     281.25   720.    1361.25]\n"
     ]
    }
   ],
   "source": [
    "def linear(inp, weights, bias):\n",
    "    return tf.matmul(inp, weights) + bias\n",
    "\n",
    "x = tf.placeholder(tf.float32, shape=[4, 3], name=\"x\")\n",
    "w = tf.constant(np.array([[1,2],[1,2],[1,2]], dtype=np.float32), name=\"w\")\n",
    "b = tf.constant(np.array([0,0], dtype=np.float32), name=\"b\")\n",
    "y = linear(x, w, b)\n",
    "\n",
    "target = tf.placeholder(tf.float32, shape=[4, 2], name=\"target\")\n",
    "\n",
    "cost = tf.reduce_sum(tf.square(y-target), axis=1)\n",
    "\n",
    "session = tf.Session()\n",
    "\n",
    "feed_dict = {\n",
    "    x: np.array([[1,2,3], [4,5,6], [7,8,9], [10,11,12]]),\n",
    "    target: np.array([[3,6], [7.5, 15], [12, 24], [16.5, 33]])\n",
    "}\n",
    "result = session.run(cost, feed_dict=feed_dict)\n",
    "print(\"cost = {}\".format(result))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another thing we are missing we that the parameters $w$ and $b$ that we want to optimized are defined as a constant in the graph, so we can't change them! What we want is to define $w$ and $b$ as a variables instead: tf.Variable.\n",
    "One thing we have to bear in mind is that unlike the constant, a Variable node has no value before it is initialized (its like being empty). So we have to run session.run(theta.initializer) before running operations that depend on it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cost = [   45.     281.25   720.    1361.25]\n"
     ]
    }
   ],
   "source": [
    "def linear(inp, weights, bias):\n",
    "    return tf.matmul(inp, weights) + bias\n",
    "\n",
    "x = tf.placeholder(tf.float32, shape=[4, 3], name=\"x\")\n",
    "w = tf.Variable(np.array([[1,2],[1,2],[1,2]], dtype=np.float32), name=\"w\")\n",
    "b = tf.Variable(np.array([0,0], dtype=np.float32), name=\"b\")\n",
    "y = linear(x, w, b)\n",
    "\n",
    "target = tf.placeholder(tf.float32, shape=[4, 2], name=\"target\")\n",
    "\n",
    "cost = tf.reduce_sum(tf.square(y-target), axis=1)\n",
    "\n",
    "session = tf.Session()\n",
    "session.run(w.initializer)\n",
    "session.run(b.initializer)\n",
    "\n",
    "feed_dict = {\n",
    "    x: np.array([[1,2,3], [4,5,6], [7,8,9], [10,11,12]]),\n",
    "    target: np.array([[3,6], [7.5, 15], [12, 24], [16.5, 33]])\n",
    "}\n",
    "result = session.run(cost, feed_dict=feed_dict)\n",
    "print(\"cost = {}\".format(result))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! Now we have inputs, outputs, targets, parameters and the cost. What else do we need? We need to know how to change $\\theta$ in order to make the cost smaller. \n",
    "__Explain what the cost is here, and how to perform gradient descent__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by computing the gradient of the cost with respect to all our parameters, which are $w$ and $b$. Tensorflow provides a way to access all variables by calling tf.global_variables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "def linear(inp, weights, bias):\n",
    "    return tf.matmul(inp, weights) + bias\n",
    "\n",
    "x = tf.placeholder(tf.float32, shape=[4, 3], name=\"x\")\n",
    "w = tf.Variable(np.array([[1,2],[1,2],[1,2]], dtype=np.float32), name=\"w\")\n",
    "b = tf.Variable(np.array([0,0], dtype=np.float32), name=\"b\")\n",
    "y = linear(x, w, b)\n",
    "\n",
    "target = tf.placeholder(tf.float32, shape=[4, 2], name=\"target\")\n",
    "\n",
    "cost = tf.reduce_mean(tf.reduce_sum(tf.square(y-target), axis=1))\n",
    "gradients = tf.gradients(cost, tf.trainable_variables())\n",
    "\n",
    "session = tf.Session()\n",
    "session.run(w.initializer)\n",
    "session.run(b.initializer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cost = 601.875\n",
      "gradients = [array([[ 141. ,  282. ],\n",
      "       [ 160.5,  321. ],\n",
      "       [ 180. ,  360. ]], dtype=float32), array([ 19.5,  39. ], dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "feed_dict = {\n",
    "    x: np.array([[1,2,3], [4,5,6], [7,8,9], [10,11,12]]),\n",
    "    target: np.array([[3,6], [7.5, 15], [12, 24], [16.5, 33]])\n",
    "}\n",
    "result = session.run([cost, gradients], feed_dict=feed_dict)\n",
    "print(\"cost = {}\".format(result[0]))\n",
    "print(\"gradients = {}\".format(result[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have to update our parameters. Since Variables are special in that their value does not depend on the input, and if we change their value we want it to remain for the next sess.run's, there are special method to update them. So, according to gradient descent we want to perform __formula__\n",
    "We can change the value using tf.assign. If we wanted to change a single variable, we would do tf.assign(var, new_value), since want to update all the variables, lets use it like this instead\n",
    "[tf.assign(var, var - 0.001 * grad) for var, grad in zip(tf.trainable_variables(), gradients)]\n",
    "just copy it to your code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "def linear(inp, weights, bias):\n",
    "    return tf.matmul(inp, weights) + bias\n",
    "\n",
    "x = tf.placeholder(tf.float32, shape=[4, 3], name=\"x\")\n",
    "w = tf.Variable(np.array([[1,2],[1,2],[1,2]], dtype=np.float32), name=\"w\")\n",
    "b = tf.Variable(np.array([0,0], dtype=np.float32), name=\"b\")\n",
    "y = linear(x, w, b)\n",
    "\n",
    "target = tf.placeholder(tf.float32, shape=[4, 2], name=\"target\")\n",
    "\n",
    "cost = tf.reduce_mean(tf.reduce_sum(tf.square(y-target), axis=1))\n",
    "gradients = tf.gradients(cost, tf.trainable_variables())\n",
    "\n",
    "update_op = tf.group(*[tf.assign(var, var - grad) for var, grad in zip(tf.trainable_variables(), gradients)])\n",
    "\n",
    "session = tf.Session()\n",
    "session.run(w.initializer)\n",
    "session.run(b.initializer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cost = 63474308.0\n"
     ]
    }
   ],
   "source": [
    "feed_dict = {\n",
    "    x: np.array([[1,2,3], [4,5,6], [7,8,9], [10,11,12]]),\n",
    "    target: np.array([[3,6], [7.5, 15], [12, 24], [16.5, 33]])\n",
    "}\n",
    "result = session.run([cost, gradients, update_op], feed_dict=feed_dict)\n",
    "print(\"cost = {}\".format(result[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we don't want to press enter everytime, let's put this session.run cell into a for loop and run it for, say, 20 iterations, printing out the cost (you don't need to print the gradient)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "def linear(inp, weights, bias):\n",
    "    return tf.matmul(inp, weights) + bias\n",
    "\n",
    "x = tf.placeholder(tf.float32, shape=[4, 3], name=\"x\")\n",
    "w = tf.Variable(np.array([[1,2],[1,2],[1,2]], dtype=np.float32), name=\"w\")\n",
    "b = tf.Variable(np.array([0,0], dtype=np.float32), name=\"b\")\n",
    "y = linear(x, w, b)\n",
    "\n",
    "target = tf.placeholder(tf.float32, shape=[4, 2], name=\"target\")\n",
    "\n",
    "cost = tf.reduce_mean(tf.reduce_sum(tf.square(y-target), axis=1))\n",
    "gradients = tf.gradients(cost, tf.trainable_variables())\n",
    "\n",
    "update_op = tf.group(*[tf.assign(var, var - grad) for var, grad in zip(tf.trainable_variables(), gradients)])\n",
    "\n",
    "session = tf.Session()\n",
    "session.run(w.initializer)\n",
    "session.run(b.initializer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cost = 601.8750\n",
      "cost = 63474308.0000\n",
      "cost = 6694441582592.0000\n",
      "cost = 706042320805429248.0000\n",
      "cost = 74464124024010752131072.0000\n",
      "cost = 7853504425729086972177154048.0000\n",
      "cost = 828285176903947277824201001533440.0000\n",
      "cost = inf\n",
      "cost = inf\n",
      "cost = inf\n",
      "cost = inf\n",
      "cost = inf\n",
      "cost = inf\n",
      "cost = inf\n",
      "cost = inf\n",
      "cost = inf\n",
      "cost = inf\n",
      "cost = nan\n",
      "cost = nan\n",
      "cost = nan\n"
     ]
    }
   ],
   "source": [
    "feed_dict = {\n",
    "    x: np.array([[1,2,3], [4,5,6], [7,8,9], [10,11,12]]),\n",
    "    target: np.array([[3,6], [7.5, 15], [12, 24], [16.5, 33]])\n",
    "}\n",
    "\n",
    "for i in range(20):\n",
    "    result = session.run([cost, gradients, update_op], feed_dict=feed_dict)\n",
    "    print(\"cost = {:.4f}\".format(result[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Oh no, our cost is going up! Does anyone know why that is happening?\n",
    "__lengthy discussion about learning rate here__\n",
    "We need to use a smaller learning rate, so go ahead and multiply the gradient by a small number until you find one that makes the cost go to zero. (settle for 0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "def linear(inp, weights, bias):\n",
    "    return tf.matmul(inp, weights) + bias\n",
    "\n",
    "x = tf.placeholder(tf.float32, shape=[4, 3], name=\"x\")\n",
    "w = tf.Variable(np.array([[1,2],[1,2],[1,2]], dtype=np.float32), name=\"w\")\n",
    "b = tf.Variable(np.array([0,0], dtype=np.float32), name=\"b\")\n",
    "y = linear(x, w, b)\n",
    "\n",
    "target = tf.placeholder(tf.float32, shape=[4, 2], name=\"target\")\n",
    "\n",
    "cost = tf.reduce_mean(tf.reduce_sum(tf.square(y-target), axis=1))\n",
    "gradients = tf.gradients(cost, tf.trainable_variables())\n",
    "\n",
    "update_op = tf.group(*[tf.assign(var, var - 0.001 * grad) for var, grad in zip(tf.trainable_variables(), gradients)])\n",
    "\n",
    "session = tf.Session()\n",
    "session.run(w.initializer)\n",
    "session.run(b.initializer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cost = 601.8750\n",
      "cost = 273.6334\n",
      "cost = 124.4133\n",
      "cost = 56.5772\n",
      "cost = 25.7386\n",
      "cost = 11.7193\n",
      "cost = 5.3459\n",
      "cost = 2.4486\n",
      "cost = 1.1314\n",
      "cost = 0.5325\n",
      "cost = 0.2602\n",
      "cost = 0.1364\n",
      "cost = 0.0801\n",
      "cost = 0.0544\n",
      "cost = 0.0427\n",
      "cost = 0.0373\n",
      "cost = 0.0348\n",
      "cost = 0.0337\n",
      "cost = 0.0331\n",
      "cost = 0.0328\n"
     ]
    }
   ],
   "source": [
    "feed_dict = {\n",
    "    x: np.array([[1,2,3], [4,5,6], [7,8,9], [10,11,12]]),\n",
    "    target: np.array([[3,6], [7.5, 15], [12, 24], [16.5, 33]])\n",
    "}\n",
    "\n",
    "for i in range(20):\n",
    "    result = session.run([cost, gradients, update_op], feed_dict=feed_dict)\n",
    "    print(\"cost = {:.4f}\".format(result[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So there it is, you just implemented your own linear regression in Tensorflow! Pat yourselves in the back and let's get right into using this to build neural networks.\n",
    "\n",
    "First of all, we need a better dataset, this one is not even real! So to start off, we are going to use the Iris petal dataset __describe dataset__\n",
    "This is a classification problem, and right now what we are doing is regression, so before we start using our actual dataset let's take a moment to talk about classification and logistic regression.\n",
    "\n",
    "__lengthy discussion of logistic regression and softmax__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, now that we understand how logistic regression works, lets change our code to perform classification. First, change your target outputs to these __provide targets__ and create a negative loglikelihood cost function __as formula shown__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "def negloglikelihood_cost(out, target_out):\n",
    "    return -tf.reduce_sum(target_out * tf.log(out + 1e-7) + (1.0 - target_out) * tf.log(1.0 - out + 1e-7), axis=1)\n",
    "\n",
    "def linear(inp, weights, bias):\n",
    "    return tf.matmul(inp, weights) + bias\n",
    "\n",
    "x = tf.placeholder(tf.float32, shape=[4, 3], name=\"x\")\n",
    "w = tf.Variable(np.array([[1,2],[1,2],[1,2]], dtype=np.float32), name=\"w\")\n",
    "b = tf.Variable(np.array([0,0], dtype=np.float32), name=\"b\")\n",
    "y = tf.nn.softmax(linear(x, w, b), 1)\n",
    "\n",
    "target = tf.placeholder(tf.float32, shape=[4, 2], name=\"target\")\n",
    "\n",
    "cost = tf.reduce_mean(negloglikelihood_cost(y, target))\n",
    "gradients = tf.gradients(cost, tf.trainable_variables())\n",
    "\n",
    "update_op = tf.group(*[tf.assign(var, var - 0.001 * grad) for var, grad in zip(tf.trainable_variables(), gradients)])\n",
    "\n",
    "session = tf.Session()\n",
    "session.run(w.initializer)\n",
    "session.run(b.initializer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cost = 16.1192\n",
      "cost = 16.1192\n",
      "cost = 16.1192\n",
      "cost = 16.1192\n",
      "cost = 16.1192\n",
      "cost = 16.1192\n",
      "cost = 16.1192\n",
      "cost = 16.1192\n",
      "cost = 16.1192\n",
      "cost = 16.1192\n",
      "cost = 16.1192\n",
      "cost = 16.1192\n",
      "cost = 16.1192\n",
      "cost = 16.1192\n",
      "cost = 16.1192\n",
      "cost = 16.1192\n",
      "cost = 16.1192\n",
      "cost = 16.1192\n",
      "cost = 16.1192\n",
      "cost = 16.1192\n"
     ]
    }
   ],
   "source": [
    "feed_dict = {\n",
    "    x: np.array([[1,2,3], [4,5,6], [7,8,9], [10,11,12]]),\n",
    "    target: np.array([[0,1], [0, 1], [1, 0], [1, 0]])\n",
    "}\n",
    "\n",
    "for i in range(20):\n",
    "    result = session.run([cost, gradients, update_op], feed_dict=feed_dict)\n",
    "    print(\"cost = {:.4f}\".format(result[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, our cost is going down, but it is doing so kinda slowly... This highlights a very important aspect of training neural networks: initializations and dimensionality matter a lot!\n",
    "__exaplain that weights should be small and inputs should be centered, or at least scaled__\n",
    "So lets instead initialize the weight matrix with values 10 times smaller and also divide the input vector by 12 (let's assume 12 is the maximum value), and see how things work now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "def negloglikelihood_cost(out, target_out):\n",
    "    return -tf.reduce_sum(target_out * tf.log(out + 1e-7) + (1.0 - target_out) * tf.log(1.0 - out + 1e-7), axis=1)\n",
    "\n",
    "def linear(inp, weights, bias):\n",
    "    return tf.matmul(inp, weights) + bias\n",
    "\n",
    "x = tf.placeholder(tf.float32, shape=[4, 3], name=\"x\")\n",
    "w = tf.Variable(np.array([[1,2],[1,2],[1,2]], dtype=np.float32)/10, name=\"w\")\n",
    "b = tf.Variable(np.array([0,0], dtype=np.float32), name=\"b\")\n",
    "y = tf.nn.softmax(linear(x, w, b), 1)\n",
    "target = tf.placeholder(tf.float32, shape=[4, 2], name=\"target\")\n",
    "\n",
    "cost = tf.reduce_mean(negloglikelihood_cost(y, target))\n",
    "gradients = tf.gradients(cost, tf.trainable_variables())\n",
    "\n",
    "update_op = tf.group(*[tf.assign(var, var - 0.001 * grad) for var, grad in zip(tf.trainable_variables(), gradients)])\n",
    "\n",
    "session = tf.Session()\n",
    "session.run(w.initializer)\n",
    "session.run(b.initializer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cost = 1.4696\n",
      "cost = 1.4691\n",
      "cost = 1.4685\n",
      "cost = 1.4679\n",
      "cost = 1.4674\n",
      "cost = 1.4668\n",
      "cost = 1.4662\n",
      "cost = 1.4657\n",
      "cost = 1.4651\n",
      "cost = 1.4645\n",
      "cost = 1.4640\n",
      "cost = 1.4634\n",
      "cost = 1.4629\n",
      "cost = 1.4623\n",
      "cost = 1.4618\n",
      "cost = 1.4612\n",
      "cost = 1.4607\n",
      "cost = 1.4601\n",
      "cost = 1.4596\n",
      "cost = 1.4590\n"
     ]
    }
   ],
   "source": [
    "feed_dict = {\n",
    "    x: np.array([[1,2,3], [4,5,6], [7,8,9], [10,11,12]])/12.,\n",
    "    target: np.array([[0,1], [0, 1], [1, 0], [1, 0]])\n",
    "}\n",
    "\n",
    "for i in range(20):\n",
    "    result = session.run([cost, gradients, update_op], feed_dict=feed_dict)\n",
    "    print(\"cost = {:.4f}\".format(result[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, now the cost is much lower, even before we start training, and thats great! However, it is still going down kinda slow. Any idea why that might be? __audience input__ Yes, now our learning rate is too small! As you see, the learning rate has to be balanced according to the usual dimensionality of our weights and outputs, and this takes some magic to set sometimes, although there are smarted methods to set them, as we will see later.\n",
    "Set your learning rate to 0.1 and run again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "def negloglikelihood_cost(out, target_out):\n",
    "    return -tf.reduce_sum(target_out * tf.log(out + 1e-7) + (1.0 - target_out) * tf.log(1.0 - out + 1e-7), axis=1)\n",
    "\n",
    "def linear(inp, weights, bias):\n",
    "    return tf.matmul(inp, weights) + bias\n",
    "\n",
    "x = tf.placeholder(tf.float32, shape=[4, 3], name=\"x\")\n",
    "w = tf.Variable(np.array([[1,2],[1,2],[1,2]], dtype=np.float32)/10, name=\"w\")\n",
    "b = tf.Variable(np.array([0,0], dtype=np.float32), name=\"b\")\n",
    "y = tf.nn.softmax(linear(x, w, b), 1)\n",
    "target = tf.placeholder(tf.float32, shape=[4, 2], name=\"target\")\n",
    "\n",
    "cost = tf.reduce_mean(negloglikelihood_cost(y, target))\n",
    "gradients = tf.gradients(cost, tf.trainable_variables())\n",
    "\n",
    "update_op = tf.group(*[tf.assign(var, var - 1.0 * grad) for var, grad in zip(tf.trainable_variables(), gradients)])\n",
    "\n",
    "session = tf.Session()\n",
    "session.run(w.initializer)\n",
    "session.run(b.initializer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cost = 1.3696\n",
      "cost = 1.2155\n",
      "cost = 1.0832\n",
      "cost = 0.9888\n",
      "cost = 0.9106\n",
      "cost = 0.8478\n",
      "cost = 0.7952\n",
      "cost = 0.7510\n",
      "cost = 0.7133\n",
      "cost = 0.6806\n",
      "cost = 0.6521\n",
      "cost = 0.6268\n",
      "cost = 0.6043\n",
      "cost = 0.5839\n",
      "cost = 0.5655\n",
      "cost = 0.5487\n",
      "cost = 0.5333\n",
      "cost = 0.5191\n",
      "cost = 0.5059\n",
      "cost = 0.4937\n"
     ]
    }
   ],
   "source": [
    "feed_dict = {\n",
    "    x: np.array([[1,2,3], [4,5,6], [7,8,9], [10,11,12]])/12.,\n",
    "    target: np.array([[0,1], [0, 1], [0, 1], [1, 0]])\n",
    "}\n",
    "\n",
    "for i in range(20):\n",
    "    result = session.run([cost, gradients, update_op], feed_dict=feed_dict)\n",
    "    print(\"cost = {:.4f}\".format(result[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great, now our cost is decreasing steadily. \n",
    "Let's just add another metric to measure the progress of our training. Since now we have classes, we can calculate the accuracy, which is easier to interpret than that weird negative loglikelihood quantity.\n",
    "Just copy the following function to your code, and put it in the fetch list alongside the cost and the rest (and print it)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "def negloglikelihood_cost(out, target_out):\n",
    "    return -tf.reduce_sum(target_out * tf.log(out + 1e-7) + (1.0 - target_out) * tf.log(1.0 - out + 1e-7), axis=1)\n",
    "\n",
    "def accuracy(out, target_out):\n",
    "    correct = tf.count_nonzero(tf.cast(tf.equal(tf.argmax(out, axis=1), tf.argmax(target_out, axis=1)), tf.float32))\n",
    "    total = out.get_shape()[0]\n",
    "    return correct/total * 100\n",
    "\n",
    "def linear(inp, weights, bias):\n",
    "    return tf.matmul(inp, weights) + bias\n",
    "\n",
    "x = tf.placeholder(tf.float32, shape=[4, 3], name=\"x\")\n",
    "w = tf.Variable(np.array([[1,2],[1,2],[1,2]], dtype=np.float32)/10, name=\"w\")\n",
    "b = tf.Variable(np.array([0,0], dtype=np.float32), name=\"b\")\n",
    "y = tf.nn.softmax(linear(x, w, b), 1)\n",
    "\n",
    "target = tf.placeholder(tf.float32, shape=[4, 2], name=\"target\")\n",
    "\n",
    "cost = tf.reduce_mean(negloglikelihood_cost(y, target))\n",
    "acc = accuracy(y, target)\n",
    "gradients = tf.gradients(cost, tf.trainable_variables())\n",
    "\n",
    "update_op = tf.group(*[tf.assign(var, var - 1.0 * grad) for var, grad in zip(tf.trainable_variables(), gradients)])\n",
    "\n",
    "session = tf.Session()\n",
    "session.run(w.initializer)\n",
    "session.run(b.initializer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cost = 1.4696, acc = 50.0%\n",
      "cost = 1.2767, acc = 50.0%\n",
      "cost = 1.0922, acc = 100.0%\n",
      "cost = 0.9705, acc = 75.0%\n",
      "cost = 0.8672, acc = 100.0%\n",
      "cost = 0.7912, acc = 75.0%\n",
      "cost = 0.7302, acc = 100.0%\n",
      "cost = 0.6809, acc = 100.0%\n",
      "cost = 0.6395, acc = 100.0%\n",
      "cost = 0.6042, acc = 100.0%\n",
      "cost = 0.5735, acc = 100.0%\n",
      "cost = 0.5465, acc = 100.0%\n",
      "cost = 0.5227, acc = 100.0%\n",
      "cost = 0.5014, acc = 100.0%\n",
      "cost = 0.4823, acc = 100.0%\n",
      "cost = 0.4650, acc = 100.0%\n",
      "cost = 0.4492, acc = 100.0%\n",
      "cost = 0.4348, acc = 100.0%\n",
      "cost = 0.4216, acc = 100.0%\n",
      "cost = 0.4094, acc = 100.0%\n"
     ]
    }
   ],
   "source": [
    "feed_dict = {\n",
    "    x: np.array([[1,2,3], [4,5,6], [7,8,9], [10,11,12]])/12.,\n",
    "    target: np.array([[0,1], [0, 1], [1, 0], [1, 0]])\n",
    "}\n",
    "\n",
    "for i in range(20):\n",
    "    result = session.run([cost, acc, gradients, update_op], feed_dict=feed_dict)\n",
    "    print(\"cost = {:.4f}, acc = {}%\".format(result[0], result[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now feel free to take a few minutes to run more iterations and see that the cost goes down, and to mess with the learning rate to see how that affects the speed of convergence. __wait 5 minutes and ask for questions from the audience__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, now lets do a tiny change to our dataset: change the third target from [1,0] to [0,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "def negloglikelihood_cost(out, target_out):\n",
    "    return -tf.reduce_sum(target_out * tf.log(out + 1e-7) + (1.0 - target_out) * tf.log(1.0 - out + 1e-7), axis=1)\n",
    "\n",
    "def accuracy(out, target_out):\n",
    "    correct = tf.count_nonzero(tf.cast(tf.equal(tf.argmax(out, axis=1), tf.argmax(target_out, axis=1)), tf.float32))\n",
    "    total = out.get_shape()[0]\n",
    "    return correct/total * 100\n",
    "\n",
    "def linear(inp, weights, bias):\n",
    "    return tf.matmul(inp, weights) + bias\n",
    "\n",
    "x = tf.placeholder(tf.float32, shape=[4, 3], name=\"x\")\n",
    "w = tf.Variable(np.array([[1,2],[1,2],[1,2]], dtype=np.float32)/10, name=\"w\")\n",
    "b = tf.Variable(np.array([0,0], dtype=np.float32), name=\"b\")\n",
    "y = tf.nn.softmax(linear(x, w, b), 1)\n",
    "\n",
    "target = tf.placeholder(tf.float32, shape=[4, 2], name=\"target\")\n",
    "\n",
    "cost = tf.reduce_mean(negloglikelihood_cost(y, target))\n",
    "acc = accuracy(y, target)\n",
    "gradients = tf.gradients(cost, tf.trainable_variables())\n",
    "\n",
    "update_op = tf.group(*[tf.assign(var, var - 1.0 * grad) for var, grad in zip(tf.trainable_variables(), gradients)])\n",
    "\n",
    "session = tf.Session()\n",
    "session.run(w.initializer)\n",
    "session.run(b.initializer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cost = 1.1840, acc = 50.0%\n",
      "cost = 1.1826, acc = 50.0%\n",
      "cost = 1.1814, acc = 50.0%\n",
      "cost = 1.1803, acc = 50.0%\n",
      "cost = 1.1794, acc = 50.0%\n",
      "cost = 1.1786, acc = 50.0%\n",
      "cost = 1.1779, acc = 50.0%\n",
      "cost = 1.1773, acc = 50.0%\n",
      "cost = 1.1768, acc = 50.0%\n",
      "cost = 1.1763, acc = 50.0%\n",
      "cost = 1.1759, acc = 50.0%\n",
      "cost = 1.1756, acc = 50.0%\n",
      "cost = 1.1753, acc = 50.0%\n",
      "cost = 1.1751, acc = 50.0%\n",
      "cost = 1.1749, acc = 50.0%\n",
      "cost = 1.1747, acc = 50.0%\n",
      "cost = 1.1746, acc = 50.0%\n",
      "cost = 1.1744, acc = 50.0%\n",
      "cost = 1.1743, acc = 50.0%\n",
      "cost = 1.1743, acc = 50.0%\n",
      "cost = 1.1742, acc = 50.0%\n",
      "cost = 1.1741, acc = 50.0%\n",
      "cost = 1.1741, acc = 50.0%\n",
      "cost = 1.1740, acc = 50.0%\n",
      "cost = 1.1740, acc = 50.0%\n",
      "cost = 1.1739, acc = 50.0%\n",
      "cost = 1.1739, acc = 50.0%\n",
      "cost = 1.1739, acc = 50.0%\n",
      "cost = 1.1739, acc = 50.0%\n",
      "cost = 1.1738, acc = 50.0%\n",
      "cost = 1.1738, acc = 50.0%\n",
      "cost = 1.1738, acc = 50.0%\n",
      "cost = 1.1738, acc = 50.0%\n",
      "cost = 1.1738, acc = 50.0%\n",
      "cost = 1.1738, acc = 50.0%\n",
      "cost = 1.1738, acc = 50.0%\n",
      "cost = 1.1738, acc = 50.0%\n",
      "cost = 1.1738, acc = 50.0%\n",
      "cost = 1.1738, acc = 50.0%\n",
      "cost = 1.1738, acc = 50.0%\n",
      "cost = 1.1738, acc = 50.0%\n",
      "cost = 1.1738, acc = 50.0%\n",
      "cost = 1.1738, acc = 50.0%\n",
      "cost = 1.1738, acc = 50.0%\n",
      "cost = 1.1738, acc = 50.0%\n",
      "cost = 1.1737, acc = 50.0%\n",
      "cost = 1.1737, acc = 50.0%\n",
      "cost = 1.1737, acc = 50.0%\n",
      "cost = 1.1737, acc = 50.0%\n",
      "cost = 1.1737, acc = 50.0%\n",
      "cost = 1.1737, acc = 50.0%\n",
      "cost = 1.1737, acc = 50.0%\n",
      "cost = 1.1737, acc = 50.0%\n",
      "cost = 1.1737, acc = 50.0%\n",
      "cost = 1.1737, acc = 50.0%\n",
      "cost = 1.1737, acc = 50.0%\n",
      "cost = 1.1737, acc = 50.0%\n",
      "cost = 1.1737, acc = 50.0%\n",
      "cost = 1.1737, acc = 50.0%\n",
      "cost = 1.1737, acc = 50.0%\n",
      "cost = 1.1737, acc = 50.0%\n",
      "cost = 1.1737, acc = 50.0%\n",
      "cost = 1.1737, acc = 50.0%\n",
      "cost = 1.1737, acc = 50.0%\n",
      "cost = 1.1737, acc = 50.0%\n",
      "cost = 1.1737, acc = 50.0%\n",
      "cost = 1.1737, acc = 50.0%\n",
      "cost = 1.1737, acc = 50.0%\n",
      "cost = 1.1737, acc = 50.0%\n",
      "cost = 1.1737, acc = 50.0%\n",
      "cost = 1.1737, acc = 50.0%\n",
      "cost = 1.1737, acc = 50.0%\n",
      "cost = 1.1737, acc = 50.0%\n",
      "cost = 1.1737, acc = 50.0%\n",
      "cost = 1.1737, acc = 50.0%\n",
      "cost = 1.1737, acc = 50.0%\n",
      "cost = 1.1737, acc = 50.0%\n",
      "cost = 1.1737, acc = 50.0%\n",
      "cost = 1.1737, acc = 50.0%\n",
      "cost = 1.1737, acc = 50.0%\n",
      "cost = 1.1737, acc = 50.0%\n",
      "cost = 1.1737, acc = 50.0%\n",
      "cost = 1.1737, acc = 50.0%\n",
      "cost = 1.1737, acc = 50.0%\n",
      "cost = 1.1737, acc = 50.0%\n",
      "cost = 1.1737, acc = 50.0%\n",
      "cost = 1.1737, acc = 50.0%\n",
      "cost = 1.1737, acc = 50.0%\n",
      "cost = 1.1737, acc = 50.0%\n",
      "cost = 1.1737, acc = 50.0%\n",
      "cost = 1.1737, acc = 50.0%\n",
      "cost = 1.1737, acc = 50.0%\n",
      "cost = 1.1737, acc = 50.0%\n",
      "cost = 1.1737, acc = 50.0%\n",
      "cost = 1.1737, acc = 50.0%\n",
      "cost = 1.1737, acc = 50.0%\n",
      "cost = 1.1737, acc = 50.0%\n",
      "cost = 1.1737, acc = 50.0%\n",
      "cost = 1.1737, acc = 50.0%\n",
      "cost = 1.1737, acc = 50.0%\n"
     ]
    }
   ],
   "source": [
    "feed_dict = {\n",
    "    x: np.array([[1,2,3], [4,5,6], [7,8,9], [10,11,12]])/12.,\n",
    "    target: np.array([[0,1], [1, 0], [0, 1], [1, 0]])\n",
    "}\n",
    "\n",
    "for i in range(100):\n",
    "    result = session.run([cost, acc, gradients, update_op], feed_dict=feed_dict)\n",
    "    print(\"cost = {:.4f}, acc = {}%\".format(result[0], result[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
